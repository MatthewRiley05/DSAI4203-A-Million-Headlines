{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16b9a063",
   "metadata": {},
   "source": [
    "### 1_data_cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a62345ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../abcnews-date-text.csv\")\n",
    "df[df.columns[0]] = pd.to_datetime(df[df.columns[0]], format=\"%Y%m%d\", errors=\"coerce\")\n",
    "df_stemmed = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bf59b7",
   "metadata": {},
   "source": [
    "### 2_feature_engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37020c3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publish_date</th>\n",
       "      <th>headline_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2003-02-19</td>\n",
       "      <td>aba decides against community broadcasting lic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2003-02-19</td>\n",
       "      <td>act fire witnesses must be aware of defamation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2003-02-19</td>\n",
       "      <td>a g calls for infrastructure protection summit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2003-02-19</td>\n",
       "      <td>air nz staff in aust strike for pay rise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2003-02-19</td>\n",
       "      <td>air nz strike to affect australian travellers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1244179</th>\n",
       "      <td>2021-12-31</td>\n",
       "      <td>two aged care residents die as state records ;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1244180</th>\n",
       "      <td>2021-12-31</td>\n",
       "      <td>victoria records ; new cases and seven deaths</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1244181</th>\n",
       "      <td>2021-12-31</td>\n",
       "      <td>wa delays adopting new close contact definition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1244182</th>\n",
       "      <td>2021-12-31</td>\n",
       "      <td>western ringtail possums found badly dehydrate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1244183</th>\n",
       "      <td>2021-12-31</td>\n",
       "      <td>what makes you a close covid contact here are ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1244184 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        publish_date                                      headline_text\n",
       "0         2003-02-19  aba decides against community broadcasting lic...\n",
       "1         2003-02-19     act fire witnesses must be aware of defamation\n",
       "2         2003-02-19     a g calls for infrastructure protection summit\n",
       "3         2003-02-19           air nz staff in aust strike for pay rise\n",
       "4         2003-02-19      air nz strike to affect australian travellers\n",
       "...              ...                                                ...\n",
       "1244179   2021-12-31     two aged care residents die as state records ;\n",
       "1244180   2021-12-31      victoria records ; new cases and seven deaths\n",
       "1244181   2021-12-31    wa delays adopting new close contact definition\n",
       "1244182   2021-12-31  western ringtail possums found badly dehydrate...\n",
       "1244183   2021-12-31  what makes you a close covid contact here are ...\n",
       "\n",
       "[1244184 rows x 2 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "\n",
    "text_col = df.columns[1]\n",
    "df[text_col] = df[text_col].apply(\n",
    "    lambda x: re.sub(r\"\\s+\", \" \", re.sub(r\"\\d+\", \"\", x.lower())).strip()\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8900b60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the TF-IDF matrix\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=10000,\n",
    "    min_df=10,\n",
    "    max_df=0.5,\n",
    "    ngram_range=(1, 3),\n",
    "    stop_words=\"english\",\n",
    "    token_pattern=r\"(?u)\\b[a-zA-Z]{2,}\\b\",\n",
    ")\n",
    "tfidf_matrix = vectorizer.fit_transform(df[text_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e215e3fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aaco' 'aaron' 'abalone' 'abandon' 'abandoned' 'abandons' 'abares'\n",
      " 'abattoir' 'abbas' 'abbot' 'abbot point' 'abbott' 'abbott says' 'abbotts'\n",
      " 'abc' 'abc business' 'abc business news' 'abc entertainment'\n",
      " 'abc learning' 'abc news' 'abc news breakfast' 'abc news quiz'\n",
      " 'abc reporter' 'abc sport' 'abc weather' 'abcs' 'abducted' 'abduction'\n",
      " 'abe' 'abetz' 'able' 'ablett' 'aboard' 'aboriginal'\n",
      " 'aboriginal community' 'aborigines' 'abortion' 'abs' 'absence' 'abu'\n",
      " 'abuse' 'abuse claims' 'abuse inquiry' 'abuse victims' 'abused' 'abuses'\n",
      " 'abusing' 'academic' 'academy' 'accc' 'accept' 'accepts' 'access'\n",
      " 'accident' 'accidental' 'accidentally' 'accidents' 'accommodation'\n",
      " 'accord' 'account' 'accounts' 'accreditation' 'accusations' 'accuse'\n",
      " 'accused' 'accused child' 'accused court' 'accused face'\n",
      " 'accused killing' 'accuses' 'aceh' 'acid' 'acknowledges' 'acl' 'acquired'\n",
      " 'acquisition' 'acquitted' 'act' 'act budget' 'act election'\n",
      " 'act government' 'act govt' 'act police' 'acted' 'acting' 'action'\n",
      " 'action group' 'actions' 'active' 'activist' 'activists' 'activity'\n",
      " 'activity analysis' 'actor' 'actors' 'actress' 'acts' 'actu' 'ad'\n",
      " 'ad campaign']\n"
     ]
    }
   ],
   "source": [
    "terms = vectorizer.get_feature_names_out()\n",
    "print(terms[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defc9822",
   "metadata": {},
   "source": [
    "### Improving\n",
    "\n",
    "As you can see, here we have words with same stem: abandon, abandoned, abandons.\n",
    "So next I will use stemming with nltk (Natural Language Toolkit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "19bc3292",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "import re\n",
    "\n",
    "stop_words = ENGLISH_STOP_WORDS\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def stem_analyzer(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"\\d+\", \" \", text)            # remove digits\n",
    "    text = re.sub(r\"[^a-z\\s]\", \" \", text)       # remove punctuation and symbols\n",
    "    text = re.sub(r\"\\s+\", \" \", text)          # normalize whitespace\n",
    "    tokens = text.split()\n",
    "    tokens = [t for t in text.split() if t not in stop_words]\n",
    "    tokens = [stemmer.stem(t) for t in tokens]\n",
    "\n",
    "    # generate 2-grams and 3-grams\n",
    "    bigrams = [tokens[i] + \" \" + tokens[i+1] for i in range(len(tokens)-1)]\n",
    "    trigrams = [tokens[i] + \" \" + tokens[i+1] + \" \" + tokens[i+2] for i in range(len(tokens) - 2)]\n",
    "    \n",
    "    return tokens + bigrams + trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4dcdd663",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_stemmed = TfidfVectorizer(\n",
    "    max_features=10000,\n",
    "    min_df=10,\n",
    "    max_df=0.5,\n",
    "    analyzer=stem_analyzer,\n",
    "    token_pattern=None  # must be None when using a custom analyzer\n",
    ")\n",
    "text_col = df_stemmed.columns[1]\n",
    "tfidf_matrix_stemmed = vectorizer_stemmed.fit_transform(df_stemmed[text_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fbd5bc77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aaco' 'aaron' 'ab' 'abalon' 'abandon' 'abar' 'abattoir' 'abba' 'abbot'\n",
      " 'abbot point' 'abbott' 'abbott say' 'abc' 'abc busi' 'abc busi news'\n",
      " 'abc entertain' 'abc learn' 'abc news' 'abc news breakfast'\n",
      " 'abc news quiz' 'abc radio' 'abc report' 'abc sport' 'abc weather'\n",
      " 'abduct' 'abe' 'abetz' 'abil' 'abl' 'ablett' 'aboard' 'abolish'\n",
      " 'aborigin' 'aborigin communiti' 'aborigin elder' 'aborigin land' 'abort'\n",
      " 'absenc' 'abu' 'abu ghraib' 'abus' 'abus alleg' 'abus case' 'abus charg'\n",
      " 'abus claim' 'abus inquiri' 'abus report' 'abus royal'\n",
      " 'abus royal commiss' 'abus survivor' 'abus victim' 'ac' 'academ'\n",
      " 'academi' 'accc' 'acceler' 'accept' 'access' 'accid' 'accident'\n",
      " 'accommod' 'accord' 'account' 'accredit' 'accus' 'accus assault'\n",
      " 'accus child' 'accus court' 'accus deni' 'accus face' 'accus face court'\n",
      " 'accus kill' 'accus murder' 'accus rape' 'ace' 'aceh' 'achiev' 'acid'\n",
      " 'acknowledg' 'acl' 'acquir' 'acquisit' 'acquit' 'acquitt' 'act'\n",
      " 'act budget' 'act elect' 'act govern' 'act govt' 'act labor' 'act polic'\n",
      " 'action' 'action group' 'activ' 'activ analysi' 'activist' 'actor'\n",
      " 'actress' 'actu' 'ad']\n"
     ]
    }
   ],
   "source": [
    "terms_stemmed = vectorizer_stemmed.get_feature_names_out()\n",
    "print(terms_stemmed[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fa87f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
