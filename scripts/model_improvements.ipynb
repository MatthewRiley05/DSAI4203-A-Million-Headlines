{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16b9a063",
   "metadata": {},
   "source": [
    "### 1_data_cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a62345ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publish_date</th>\n",
       "      <th>headline_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>116298</th>\n",
       "      <td>2004-09-20</td>\n",
       "      <td>10 killed in pakistan bus crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57967</th>\n",
       "      <td>2003-11-29</td>\n",
       "      <td>10 killed in pakistan bus crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>911080</th>\n",
       "      <td>2014-10-23</td>\n",
       "      <td>110 with barry nicholls</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>672958</th>\n",
       "      <td>2012-02-17</td>\n",
       "      <td>110 with barry nicholls</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>748629</th>\n",
       "      <td>2012-12-14</td>\n",
       "      <td>110 with barry nicholls</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       publish_date                    headline_text\n",
       "116298   2004-09-20  10 killed in pakistan bus crash\n",
       "57967    2003-11-29  10 killed in pakistan bus crash\n",
       "911080   2014-10-23          110 with barry nicholls\n",
       "672958   2012-02-17          110 with barry nicholls\n",
       "748629   2012-12-14          110 with barry nicholls"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "\n",
    "df = pd.read_csv(\"../abcnews-date-text.csv\")\n",
    "#change date format\n",
    "df[df.columns[0]] = pd.to_datetime(df[df.columns[0]], format=\"%Y%m%d\", errors=\"coerce\")\n",
    "# checlk for duplicates\n",
    "df[df['headline_text'].duplicated(keep=False)].sort_values('headline_text').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4c6d19b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicates\n",
    "df = df.drop_duplicates('headline_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4719efac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# work on a copy of the original dataframe\n",
    "df_stemmed = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bf59b7",
   "metadata": {},
   "source": [
    "### 2_feature_engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37020c3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publish_date</th>\n",
       "      <th>headline_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2003-02-19</td>\n",
       "      <td>aba decides against community broadcasting lic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2003-02-19</td>\n",
       "      <td>act fire witnesses must be aware of defamation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2003-02-19</td>\n",
       "      <td>a g calls for infrastructure protection summit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2003-02-19</td>\n",
       "      <td>air nz staff in aust strike for pay rise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2003-02-19</td>\n",
       "      <td>air nz strike to affect australian travellers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1244179</th>\n",
       "      <td>2021-12-31</td>\n",
       "      <td>two aged care residents die as state records ;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1244180</th>\n",
       "      <td>2021-12-31</td>\n",
       "      <td>victoria records ; new cases and seven deaths</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1244181</th>\n",
       "      <td>2021-12-31</td>\n",
       "      <td>wa delays adopting new close contact definition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1244182</th>\n",
       "      <td>2021-12-31</td>\n",
       "      <td>western ringtail possums found badly dehydrate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1244183</th>\n",
       "      <td>2021-12-31</td>\n",
       "      <td>what makes you a close covid contact here are ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1244184 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        publish_date                                      headline_text\n",
       "0         2003-02-19  aba decides against community broadcasting lic...\n",
       "1         2003-02-19     act fire witnesses must be aware of defamation\n",
       "2         2003-02-19     a g calls for infrastructure protection summit\n",
       "3         2003-02-19           air nz staff in aust strike for pay rise\n",
       "4         2003-02-19      air nz strike to affect australian travellers\n",
       "...              ...                                                ...\n",
       "1244179   2021-12-31     two aged care residents die as state records ;\n",
       "1244180   2021-12-31      victoria records ; new cases and seven deaths\n",
       "1244181   2021-12-31    wa delays adopting new close contact definition\n",
       "1244182   2021-12-31  western ringtail possums found badly dehydrate...\n",
       "1244183   2021-12-31  what makes you a close covid contact here are ...\n",
       "\n",
       "[1244184 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "\n",
    "text_col = df.columns[1]\n",
    "df[text_col] = df[text_col].apply(\n",
    "    lambda x: re.sub(r\"\\s+\", \" \", re.sub(r\"\\d+\", \"\", x.lower())).strip()\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8900b60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the TF-IDF matrix\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=10000,\n",
    "    min_df=10,\n",
    "    max_df=0.5,\n",
    "    ngram_range=(1, 3),\n",
    "    stop_words=\"english\",\n",
    "    token_pattern=r\"(?u)\\b[a-zA-Z]{2,}\\b\",\n",
    ")\n",
    "tfidf_matrix = vectorizer.fit_transform(df[text_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e215e3fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aaco' 'aaron' 'abalone' 'abandon' 'abandoned' 'abandons' 'abares'\n",
      " 'abattoir' 'abbas' 'abbot' 'abbot point' 'abbott' 'abbott says' 'abbotts'\n",
      " 'abc' 'abc business' 'abc business news' 'abc entertainment'\n",
      " 'abc learning' 'abc news' 'abc news breakfast' 'abc news quiz'\n",
      " 'abc reporter' 'abc sport' 'abc weather' 'abcs' 'abducted' 'abduction'\n",
      " 'abe' 'abetz' 'able' 'ablett' 'aboard' 'aboriginal'\n",
      " 'aboriginal community' 'aborigines' 'abortion' 'abs' 'absence' 'abu'\n",
      " 'abuse' 'abuse claims' 'abuse inquiry' 'abuse victims' 'abused' 'abuses'\n",
      " 'abusing' 'academic' 'academy' 'accc' 'accept' 'accepts' 'access'\n",
      " 'accident' 'accidental' 'accidentally' 'accidents' 'accommodation'\n",
      " 'accord' 'account' 'accounts' 'accreditation' 'accusations' 'accuse'\n",
      " 'accused' 'accused child' 'accused court' 'accused face'\n",
      " 'accused killing' 'accuses' 'aceh' 'acid' 'acknowledges' 'acl' 'acquired'\n",
      " 'acquisition' 'acquitted' 'act' 'act budget' 'act election'\n",
      " 'act government' 'act govt' 'act police' 'acted' 'acting' 'action'\n",
      " 'action group' 'actions' 'active' 'activist' 'activists' 'activity'\n",
      " 'activity analysis' 'actor' 'actors' 'actress' 'acts' 'actu' 'ad'\n",
      " 'ad campaign']\n"
     ]
    }
   ],
   "source": [
    "terms = vectorizer.get_feature_names_out()\n",
    "print(terms[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defc9822",
   "metadata": {},
   "source": [
    "### Improving\n",
    "\n",
    "As you can see, here we have words with same stem: abandon, abandoned, abandons.\n",
    "So next I will use stemming with nltk (Natural Language Toolkit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bc3292",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "import re\n",
    "\n",
    "stop_words = ENGLISH_STOP_WORDS\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def stem_analyzer(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"\\d+\", \" \", text)            # remove digits\n",
    "    text = re.sub(r\"[^a-z\\s]\", \" \", text)       # remove punctuation and symbols\n",
    "    text = re.sub(r\"\\s+\", \" \", text)          # normalize whitespace\n",
    "    tokens = text.split()\n",
    "    tokens = [t for t in text.split() if t not in stop_words]\n",
    "    tokens = [stemmer.stem(t) for t in tokens]\n",
    "\n",
    "    # generate 2-grams and 3-grams\n",
    "    bigrams = [tokens[i] + \" \" + tokens[i+1] for i in range(len(tokens)-1)]\n",
    "    trigrams = [tokens[i] + \" \" + tokens[i+1] + \" \" + tokens[i+2] for i in range(len(tokens) - 2)]\n",
    "    \n",
    "    return tokens + bigrams + trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcdd663",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_stemmed = TfidfVectorizer(\n",
    "    max_features=10000,\n",
    "    min_df=10,\n",
    "    max_df=0.5,\n",
    "    analyzer=stem_analyzer,\n",
    "    token_pattern=None  # must be None when using a custom analyzer\n",
    ")\n",
    "text_col = df_stemmed.columns[1]\n",
    "tfidf_matrix_stemmed = vectorizer_stemmed.fit_transform(df_stemmed[text_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd5bc77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aa' 'aaa' 'aaa credit' 'aaa credit rate' 'aaa rate' 'aaco'\n",
      " 'aaco abattoir' 'aacta' 'aacta award' 'aamer' 'aami' 'aapt' 'aaron'\n",
      " 'aaron finch' 'aaron mooy' 'aaron pajich' 'aaron pajich murder'\n",
      " 'aaron payn' 'aaron sandiland' 'aaron wood' 'ab' 'ab data' 'ab figur'\n",
      " 'ab job' 'ab june' 'ab villier' 'aba' 'abalon' 'abalon diver'\n",
      " 'abalon farm' 'abalon fish' 'abalon fisher' 'abalon haul'\n",
      " 'abalon industri' 'abalon poach' 'abalon poacher' 'abalon season'\n",
      " 'abalon virus' 'abandon' 'abandon babi' 'abandon car' 'abandon hous'\n",
      " 'abandon plan' 'abandon toddler' 'abar' 'abar crop' 'abar forecast'\n",
      " 'abar outlook' 'abar predict' 'abat' 'abattoir' 'abattoir close'\n",
      " 'abattoir closur' 'abattoir get' 'abattoir open' 'abattoir owner'\n",
      " 'abattoir plan' 'abattoir reopen' 'abattoir worker' 'abb' 'abb grain'\n",
      " 'abba' 'abbatoir' 'abbey' 'abbey road' 'abbi' 'abbot' 'abbot point'\n",
      " 'abbot point coal' 'abbot point dredg' 'abbot point expans' 'abbott'\n",
      " 'abbott accus' 'abbott address' 'abbott announc' 'abbott arriv'\n",
      " 'abbott attack' 'abbott back' 'abbott blame' 'abbott budget'\n",
      " 'abbott call' 'abbott campaign' 'abbott carbon' 'abbott carbon tax'\n",
      " 'abbott claim' 'abbott climat' 'abbott commit' 'abbott confid'\n",
      " 'abbott consid' 'abbott criticis' 'abbott defend' 'abbott deliv'\n",
      " 'abbott deni' 'abbott dismiss' 'abbott face' 'abbott flag'\n",
      " 'abbott govern' 'abbott launch' 'abbott make' 'abbott meet']\n",
      "(91607,)\n"
     ]
    }
   ],
   "source": [
    "terms_stemmed = vectorizer_stemmed.get_feature_names_out()\n",
    "print(terms_stemmed[:100])\n",
    "print(terms_stemmed.shape)\n",
    "\n",
    "# checked shape wihtout max_features parameter: 91607 features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df1220e",
   "metadata": {},
   "source": [
    "'aa' 'ab' 'ab data' 'ab figur' 'ab job' 'ab june' 'ab villier' 'aba'\n",
    "\n",
    "I think we should remove terms consisting less than 2 letters even after stemming it seems like random noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "824e2eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_analyzer2(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"\\d+\", \" \", text)            # remove digits\n",
    "    text = re.sub(r\"[^a-z\\s]\", \" \", text)       # remove punctuation and symbols\n",
    "    text = re.sub(r\"\\s+\", \" \", text)          # normalize whitespace\n",
    "    tokens = text.split()\n",
    "    tokens = [t for t in text.split() if t not in stop_words]\n",
    "    tokens = [stemmer.stem(t) for t in tokens]\n",
    "    \n",
    "    # remove short tokens consisting of less than 3 characters\n",
    "    tokens = [t for t in tokens if len(t) >= 3]\n",
    "\n",
    "    # generate 2-grams and 3-grams\n",
    "    bigrams = [tokens[i] + \" \" + tokens[i+1] for i in range(len(tokens)-1)]\n",
    "    trigrams = [tokens[i] + \" \" + tokens[i+1] + \" \" + tokens[i+2] for i in range(len(tokens) - 2)]\n",
    "    \n",
    "    return tokens + bigrams + trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0f1d9b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_stemmed = TfidfVectorizer(\n",
    "    max_features=10000,\n",
    "    min_df=10,\n",
    "    max_df=0.5,\n",
    "    analyzer=stem_analyzer2,\n",
    "    token_pattern=None  # must be None when using a custom analyzer\n",
    ")\n",
    "text_col = df_stemmed.columns[1]\n",
    "tfidf_matrix_stemmed = vectorizer_stemmed.fit_transform(df_stemmed[text_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5248218f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['aaco', 'aaron', 'abalon', 'abandon', 'abar', 'abattoir', 'abba',\n",
       "       'abbot', 'abbot point', 'abbott', 'abbott say', 'abc',\n",
       "       'abc journalist', 'abc learn', 'abc news', 'abc news breakfast',\n",
       "       'abc news quiz', 'abc radio', 'abc report', 'abduct', 'abe',\n",
       "       'abetz', 'abil', 'abl', 'ablett', 'aboard', 'abolish', 'aborigin',\n",
       "       'aborigin communiti', 'aborigin elder', 'aborigin land', 'abort',\n",
       "       'absenc', 'absolut', 'abu', 'abu ghraib', 'abus', 'abus alleg',\n",
       "       'abus case', 'abus charg', 'abus claim', 'abus inquiri',\n",
       "       'abus report', 'abus royal', 'abus royal commiss', 'abus survivor',\n",
       "       'abus victim', 'academ', 'academi', 'accc', 'acceler', 'accept',\n",
       "       'access', 'accid', 'accident', 'accommod', 'accord', 'account',\n",
       "       'accredit', 'accus', 'accus assault', 'accus child', 'accus court',\n",
       "       'accus deni', 'accus drug', 'accus face', 'accus face court',\n",
       "       'accus govt', 'accus kill', 'accus murder', 'accus rape',\n",
       "       'accus tri', 'ace', 'aceh', 'achiev', 'acid', 'acknowledg', 'acl',\n",
       "       'acquir', 'acquisit', 'acquit', 'acquitt', 'act', 'act budget',\n",
       "       'act elect', 'act govern', 'act govt', 'act labor', 'act polic',\n",
       "       'action', 'action group', 'activ', 'activist', 'actor', 'actress',\n",
       "       'actu', 'actual', 'adam', 'adam gile', 'adam good'], dtype=object)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer_stemmed.get_feature_names_out()[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a472b3ac",
   "metadata": {},
   "source": [
    "Now I think it seems better"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed02f124",
   "metadata": {},
   "source": [
    "After consideration and some noises in clustering I was thinking, I don't have enought dimension:\n",
    "\n",
    "So I increased SVD 200 -> 300: result was worse\n",
    "\n",
    "Now I am thinking maybe trigrams are causing some noise, and preventing from getting other useful features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f295d778",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "206"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigrams = [t for t in vectorizer_stemmed.get_feature_names_out()\n",
    "            if len(t.split()) == 3]\n",
    "\n",
    "len(trigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40650a06",
   "metadata": {},
   "source": [
    "We have only 206 trigrams, meaning they are few and rare, which means they are less likely affect 10K features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62d2f86",
   "metadata": {},
   "source": [
    "### 3_Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fa87f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# with open(\"../outputs/tfidf_matrix.pkl\", \"rb\") as f:\n",
    "#     tfidf_matrix = pickle.load(f)\n",
    "\n",
    "# with open(\"../outputs/tfidf_vectorizer.pkl\", \"rb\") as f:\n",
    "#     vectorizer = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83aa163",
   "metadata": {},
   "source": [
    "I wanted to check how much variance is preserved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "885d4244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 0.06846438882091532\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[51]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m [\u001b[32m50\u001b[39m, \u001b[32m100\u001b[39m, \u001b[32m200\u001b[39m, \u001b[32m300\u001b[39m]:\n\u001b[32m      2\u001b[39m     svd_k = TruncatedSVD(n_components=k, random_state=\u001b[32m42\u001b[39m, algorithm=\u001b[33m'\u001b[39m\u001b[33mrandomized\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     \u001b[43msvd_k\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtfidf_matrix_stemmed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m     \u001b[38;5;28mprint\u001b[39m(k, svd_k.explained_variance_ratio_.sum())\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\urynb\\Projects\\ML_group_project\\DSAI4203-A-Million-Headlines\\.venv\\Lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py:206\u001b[39m, in \u001b[36mTruncatedSVD.fit\u001b[39m\u001b[34m(self, X, y)\u001b[39m\n\u001b[32m    190\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    191\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Fit model on training data X.\u001b[39;00m\n\u001b[32m    192\u001b[39m \n\u001b[32m    193\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    204\u001b[39m \u001b[33;03m        Returns the transformer object.\u001b[39;00m\n\u001b[32m    205\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m206\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    207\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\urynb\\Projects\\ML_group_project\\DSAI4203-A-Million-Headlines\\.venv\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:316\u001b[39m, in \u001b[36m_wrap_method_output.<locals>.wrapped\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    314\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m     data_to_wrap = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    317\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    318\u001b[39m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    319\u001b[39m         return_tuple = (\n\u001b[32m    320\u001b[39m             _wrap_data_with_container(method, data_to_wrap[\u001b[32m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[32m    321\u001b[39m             *data_to_wrap[\u001b[32m1\u001b[39m:],\n\u001b[32m    322\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\urynb\\Projects\\ML_group_project\\DSAI4203-A-Million-Headlines\\.venv\\Lib\\site-packages\\sklearn\\base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\urynb\\Projects\\ML_group_project\\DSAI4203-A-Million-Headlines\\.venv\\Lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py:244\u001b[39m, in \u001b[36mTruncatedSVD.fit_transform\u001b[39m\u001b[34m(self, X, y)\u001b[39m\n\u001b[32m    239\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.n_components > X.shape[\u001b[32m1\u001b[39m]:\n\u001b[32m    240\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    241\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mn_components(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.n_components\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) must be <=\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    242\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m n_features(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX.shape[\u001b[32m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m).\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    243\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m244\u001b[39m     U, Sigma, VT = \u001b[43m_randomized_svd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    245\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    246\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_components\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    247\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_iter\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_oversamples\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_oversamples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpower_iteration_normalizer\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpower_iteration_normalizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m        \u001b[49m\u001b[43mflip_sign\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    253\u001b[39m     U, VT = svd_flip(U, VT, u_based_decision=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    255\u001b[39m \u001b[38;5;28mself\u001b[39m.components_ = VT\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\urynb\\Projects\\ML_group_project\\DSAI4203-A-Million-Headlines\\.venv\\Lib\\site-packages\\sklearn\\utils\\extmath.py:568\u001b[39m, in \u001b[36m_randomized_svd\u001b[39m\u001b[34m(M, n_components, n_oversamples, n_iter, power_iteration_normalizer, transpose, flip_sign, random_state, svd_lapack_driver)\u001b[39m\n\u001b[32m    564\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m transpose:\n\u001b[32m    565\u001b[39m     \u001b[38;5;66;03m# this implementation is a bit faster with smaller shape[1]\u001b[39;00m\n\u001b[32m    566\u001b[39m     M = M.T\n\u001b[32m--> \u001b[39m\u001b[32m568\u001b[39m Q = \u001b[43m_randomized_range_finder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    569\u001b[39m \u001b[43m    \u001b[49m\u001b[43mM\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    570\u001b[39m \u001b[43m    \u001b[49m\u001b[43msize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_random\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    571\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_iter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    572\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpower_iteration_normalizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpower_iteration_normalizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    573\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    574\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    576\u001b[39m \u001b[38;5;66;03m# project M to the (k + p) dimensional space using the basis vectors\u001b[39;00m\n\u001b[32m    577\u001b[39m B = Q.T @ M\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\urynb\\Projects\\ML_group_project\\DSAI4203-A-Million-Headlines\\.venv\\Lib\\site-packages\\sklearn\\utils\\extmath.py:350\u001b[39m, in \u001b[36m_randomized_range_finder\u001b[39m\u001b[34m(A, size, n_iter, power_iteration_normalizer, random_state)\u001b[39m\n\u001b[32m    347\u001b[39m \u001b[38;5;66;03m# Perform power iterations with Q to further 'imprint' the top\u001b[39;00m\n\u001b[32m    348\u001b[39m \u001b[38;5;66;03m# singular vectors of A in Q\u001b[39;00m\n\u001b[32m    349\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_iter):\n\u001b[32m--> \u001b[39m\u001b[32m350\u001b[39m     Q, _ = \u001b[43mnormalizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m \u001b[49m\u001b[43m@\u001b[49m\u001b[43m \u001b[49m\u001b[43mQ\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    351\u001b[39m     Q, _ = normalizer(A.T @ Q)\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# Sample the range of A using by linear projection of Q\u001b[39;00m\n\u001b[32m    354\u001b[39m \u001b[38;5;66;03m# Extract an orthonormal basis\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\urynb\\Projects\\ML_group_project\\DSAI4203-A-Million-Headlines\\.venv\\Lib\\site-packages\\scipy\\linalg\\_decomp_lu.py:367\u001b[39m, in \u001b[36mlu\u001b[39m\u001b[34m(a, permute_l, overwrite_a, check_finite, p_indices)\u001b[39m\n\u001b[32m    365\u001b[39m     p = np.empty(m, dtype=np.int32)\n\u001b[32m    366\u001b[39m     u = np.zeros([k, k], dtype=a1.dtype)\n\u001b[32m--> \u001b[39m\u001b[32m367\u001b[39m     \u001b[43mlu_dispatcher\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpermute_l\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    368\u001b[39m     P, L, U = (p, a1, u) \u001b[38;5;28;01mif\u001b[39;00m m > n \u001b[38;5;28;01melse\u001b[39;00m (p, u, a1)\n\u001b[32m    370\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# Stacked array\u001b[39;00m\n\u001b[32m    371\u001b[39m \n\u001b[32m    372\u001b[39m     \u001b[38;5;66;03m# Prepare the contiguous data holders\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "for k in [50, 100, 200, 300]:\n",
    "    svd_k = TruncatedSVD(n_components=k, random_state=42, algorithm='randomized')\n",
    "    svd_k.fit(tfidf_matrix_stemmed)\n",
    "    print(k, svd_k.explained_variance_ratio_.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bf66ab",
   "metadata": {},
   "source": [
    "But it works too long, so next I'll do larger k numbers but with sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1bf66212",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "n_samples = 250000\n",
    "rows = np.random.choice(tfidf_matrix_stemmed.shape[0], size=n_samples, replace=False)\n",
    "tfidf_sample = tfidf_matrix_stemmed[rows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e3e923dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=100: explained variance = 0.1180\n",
      "k=200: explained variance = 0.1946\n",
      "k=300: explained variance = 0.2556\n",
      "k=400: explained variance = 0.3065\n",
      "k=500: explained variance = 0.3501\n"
     ]
    }
   ],
   "source": [
    "ks = [100, 200, 300, 400, 500]\n",
    "results = {}\n",
    "\n",
    "for k in ks:\n",
    "    svd_k = TruncatedSVD(n_components=k, random_state=42, algorithm='randomized')\n",
    "    svd_k.fit(tfidf_sample)\n",
    "    explained = svd_k.explained_variance_ratio_.sum()\n",
    "    results[k] = explained\n",
    "    print(f\"k={k}: explained variance = {explained:.4f}\")\n",
    "\n",
    "# Storing in svd_results.txt\n",
    "with open(\"../outputs/svd_results.txt\", \"w\") as f:\n",
    "    for k, val in results.items():\n",
    "        f.write(f\"k = {k}: explained variance = {val:.6f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b7e7d4",
   "metadata": {},
   "source": [
    "This evaluation with SAMPLE gives similar results as without it, so it is accurate.\n",
    "\n",
    "However, picking up larger k number would make further computations costly: \n",
    "\n",
    "So we would stick to 200, because 300 gives extra 6% variance but with trade-off 50% more dimensions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b56c177",
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=200, random_state=42, algorithm='randomized')\n",
    "lsa = svd.fit_transform(tfidf_matrix_stemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16fd46b",
   "metadata": {},
   "source": [
    "### Trying Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe16ff52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.01826689, -0.01919928, -0.00252245, ...,  0.00578481,\n",
       "         0.02331572,  0.00167638],\n",
       "       [ 0.02275818, -0.01223092,  0.00530972, ...,  0.00580389,\n",
       "         0.00394646, -0.01437576],\n",
       "       [ 0.03100368, -0.03363738, -0.00031488, ...,  0.02387418,\n",
       "         0.02904686,  0.02196581],\n",
       "       ...,\n",
       "       [ 0.06605609, -0.10435397, -0.02448211, ...,  0.00936457,\n",
       "        -0.0023124 ,  0.00544669],\n",
       "       [ 0.00909803, -0.00780246, -0.00199412, ..., -0.00671774,\n",
       "        -0.00829798, -0.00113839],\n",
       "       [ 0.0813937 , -0.11897488, -0.02816409, ...,  0.00999959,\n",
       "        -0.01723483,  0.01807941]], shape=(1213004, 200))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2c1807",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
