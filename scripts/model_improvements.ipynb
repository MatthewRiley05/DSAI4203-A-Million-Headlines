{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16b9a063",
   "metadata": {},
   "source": [
    "### 1_data_cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a62345ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publish_date</th>\n",
       "      <th>headline_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>116298</th>\n",
       "      <td>2004-09-20</td>\n",
       "      <td>10 killed in pakistan bus crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57967</th>\n",
       "      <td>2003-11-29</td>\n",
       "      <td>10 killed in pakistan bus crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>911080</th>\n",
       "      <td>2014-10-23</td>\n",
       "      <td>110 with barry nicholls</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>672958</th>\n",
       "      <td>2012-02-17</td>\n",
       "      <td>110 with barry nicholls</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>748629</th>\n",
       "      <td>2012-12-14</td>\n",
       "      <td>110 with barry nicholls</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       publish_date                    headline_text\n",
       "116298   2004-09-20  10 killed in pakistan bus crash\n",
       "57967    2003-11-29  10 killed in pakistan bus crash\n",
       "911080   2014-10-23          110 with barry nicholls\n",
       "672958   2012-02-17          110 with barry nicholls\n",
       "748629   2012-12-14          110 with barry nicholls"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "\n",
    "df = pd.read_csv(\"../abcnews-date-text.csv\")\n",
    "#change date format\n",
    "df[df.columns[0]] = pd.to_datetime(df[df.columns[0]], format=\"%Y%m%d\", errors=\"coerce\")\n",
    "# checlk for duplicates\n",
    "df[df['headline_text'].duplicated(keep=False)].sort_values('headline_text').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c6d19b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicates\n",
    "df = df.drop_duplicates('headline_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4719efac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# work on a copy of the original dataframe\n",
    "df_stemmed = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bf59b7",
   "metadata": {},
   "source": [
    "### 2_feature_engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37020c3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publish_date</th>\n",
       "      <th>headline_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2003-02-19</td>\n",
       "      <td>aba decides against community broadcasting lic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2003-02-19</td>\n",
       "      <td>act fire witnesses must be aware of defamation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2003-02-19</td>\n",
       "      <td>a g calls for infrastructure protection summit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2003-02-19</td>\n",
       "      <td>air nz staff in aust strike for pay rise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2003-02-19</td>\n",
       "      <td>air nz strike to affect australian travellers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1244179</th>\n",
       "      <td>2021-12-31</td>\n",
       "      <td>two aged care residents die as state records ;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1244180</th>\n",
       "      <td>2021-12-31</td>\n",
       "      <td>victoria records ; new cases and seven deaths</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1244181</th>\n",
       "      <td>2021-12-31</td>\n",
       "      <td>wa delays adopting new close contact definition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1244182</th>\n",
       "      <td>2021-12-31</td>\n",
       "      <td>western ringtail possums found badly dehydrate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1244183</th>\n",
       "      <td>2021-12-31</td>\n",
       "      <td>what makes you a close covid contact here are ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1213004 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        publish_date                                      headline_text\n",
       "0         2003-02-19  aba decides against community broadcasting lic...\n",
       "1         2003-02-19     act fire witnesses must be aware of defamation\n",
       "2         2003-02-19     a g calls for infrastructure protection summit\n",
       "3         2003-02-19           air nz staff in aust strike for pay rise\n",
       "4         2003-02-19      air nz strike to affect australian travellers\n",
       "...              ...                                                ...\n",
       "1244179   2021-12-31     two aged care residents die as state records ;\n",
       "1244180   2021-12-31      victoria records ; new cases and seven deaths\n",
       "1244181   2021-12-31    wa delays adopting new close contact definition\n",
       "1244182   2021-12-31  western ringtail possums found badly dehydrate...\n",
       "1244183   2021-12-31  what makes you a close covid contact here are ...\n",
       "\n",
       "[1213004 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "\n",
    "text_col = df.columns[1]\n",
    "df[text_col] = df[text_col].apply(\n",
    "    lambda x: re.sub(r\"\\s+\", \" \", re.sub(r\"\\d+\", \"\", x.lower())).strip()\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8900b60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the TF-IDF matrix\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=10000,\n",
    "    min_df=10,\n",
    "    max_df=0.5,\n",
    "    ngram_range=(1, 3),\n",
    "    stop_words=\"english\",\n",
    "    token_pattern=r\"(?u)\\b[a-zA-Z]{2,}\\b\",\n",
    ")\n",
    "tfidf_matrix = vectorizer.fit_transform(df[text_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e215e3fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aaco' 'aaron' 'abalone' 'abandon' 'abandoned' 'abandons' 'abares'\n",
      " 'abattoir' 'abbas' 'abbot' 'abbot point' 'abbott' 'abbott says' 'abbotts'\n",
      " 'abc' 'abc learning' 'abc news' 'abc news breakfast' 'abc news quiz'\n",
      " 'abc reporter' 'abcs' 'abducted' 'abduction' 'abe' 'abetz' 'able'\n",
      " 'ablett' 'aboard' 'aboriginal' 'aboriginal community' 'aborigines'\n",
      " 'abortion' 'abs' 'absence' 'abu' 'abuse' 'abuse allegations'\n",
      " 'abuse claims' 'abuse inquiry' 'abuse victims' 'abused' 'abuses'\n",
      " 'abusing' 'academic' 'academy' 'accc' 'accept' 'accepts' 'access'\n",
      " 'accident' 'accidental' 'accidentally' 'accidents' 'accommodation'\n",
      " 'accord' 'according' 'account' 'accounts' 'accreditation' 'accusations'\n",
      " 'accuse' 'accused' 'accused child' 'accused court' 'accused face'\n",
      " 'accused killing' 'accuses' 'aceh' 'acid' 'acknowledges' 'acl' 'acquired'\n",
      " 'acquisition' 'acquitted' 'act' 'act budget' 'act election'\n",
      " 'act government' 'act govt' 'act police' 'acted' 'acting' 'action'\n",
      " 'action group' 'actions' 'active' 'activist' 'activists' 'activity'\n",
      " 'actor' 'actors' 'actress' 'acts' 'actu' 'ad' 'ad campaign' 'adam'\n",
      " 'adam giles' 'adam scott' 'adams']\n"
     ]
    }
   ],
   "source": [
    "terms = vectorizer.get_feature_names_out()\n",
    "print(terms[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defc9822",
   "metadata": {},
   "source": [
    "### Improving\n",
    "\n",
    "As you can see, here we have words with same stem: abandon, abandoned, abandons.\n",
    "So next I will use stemming with nltk (Natural Language Toolkit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19bc3292",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "import re\n",
    "\n",
    "stop_words = ENGLISH_STOP_WORDS\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def stem_analyzer(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"\\d+\", \" \", text)            # remove digits\n",
    "    text = re.sub(r\"[^a-z\\s]\", \" \", text)       # remove punctuation and symbols\n",
    "    text = re.sub(r\"\\s+\", \" \", text)          # normalize whitespace\n",
    "    tokens = text.split()\n",
    "    tokens = [t for t in text.split() if t not in stop_words]\n",
    "    tokens = [stemmer.stem(t) for t in tokens]\n",
    "\n",
    "    # generate 2-grams and 3-grams\n",
    "    bigrams = [tokens[i] + \" \" + tokens[i+1] for i in range(len(tokens)-1)]\n",
    "    trigrams = [tokens[i] + \" \" + tokens[i+1] + \" \" + tokens[i+2] for i in range(len(tokens) - 2)]\n",
    "    \n",
    "    return tokens + bigrams + trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4dcdd663",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_stemmed = TfidfVectorizer(\n",
    "    max_features=10000,\n",
    "    min_df=10,\n",
    "    max_df=0.5,\n",
    "    analyzer=stem_analyzer,\n",
    "    token_pattern=None  # must be None when using a custom analyzer\n",
    ")\n",
    "text_col = df_stemmed.columns[1]\n",
    "tfidf_matrix_stemmed = vectorizer_stemmed.fit_transform(df_stemmed[text_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fbd5bc77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aaco' 'aaron' 'ab' 'abalon' 'abandon' 'abar' 'abattoir' 'abba' 'abbot'\n",
      " 'abbot point' 'abbott' 'abbott say' 'abc' 'abc learn' 'abc news'\n",
      " 'abc news breakfast' 'abc news quiz' 'abc radio' 'abc report' 'abduct'\n",
      " 'abe' 'abetz' 'abil' 'abl' 'ablett' 'aboard' 'abolish' 'aborigin'\n",
      " 'aborigin communiti' 'aborigin elder' 'aborigin land' 'abort' 'absenc'\n",
      " 'absolut' 'abu' 'abu ghraib' 'abus' 'abus alleg' 'abus case' 'abus charg'\n",
      " 'abus claim' 'abus inquiri' 'abus report' 'abus royal'\n",
      " 'abus royal commiss' 'abus survivor' 'abus victim' 'ac' 'academ'\n",
      " 'academi' 'accc' 'acceler' 'accept' 'access' 'accid' 'accident'\n",
      " 'accommod' 'accord' 'account' 'accredit' 'accus' 'accus assault'\n",
      " 'accus child' 'accus court' 'accus deni' 'accus face' 'accus face court'\n",
      " 'accus kill' 'accus murder' 'accus rape' 'accus tri' 'ace' 'aceh'\n",
      " 'achiev' 'acid' 'acknowledg' 'acl' 'acquir' 'acquisit' 'acquit' 'acquitt'\n",
      " 'act' 'act budget' 'act elect' 'act govern' 'act govt' 'act labor'\n",
      " 'act polic' 'action' 'action group' 'activ' 'activist' 'actor' 'actress'\n",
      " 'actu' 'actual' 'ad' 'ad campaign' 'adam' 'adam gile']\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "terms_stemmed = vectorizer_stemmed.get_feature_names_out()\n",
    "print(terms_stemmed[:100])\n",
    "print(terms_stemmed.shape)\n",
    "\n",
    "# checked shape wihtout max_features parameter: 91607 features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df1220e",
   "metadata": {},
   "source": [
    "'aa' 'ab' 'ab data' 'ab figur' 'ab job' 'ab june' 'ab villier' 'aba'\n",
    "\n",
    "I think we should remove terms consisting less than 2 letters even after stemming it seems like random noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "824e2eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_analyzer2(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"\\d+\", \" \", text)            # remove digits\n",
    "    text = re.sub(r\"[^a-z\\s]\", \" \", text)       # remove punctuation and symbols\n",
    "    text = re.sub(r\"\\s+\", \" \", text)          # normalize whitespace\n",
    "    tokens = text.split()\n",
    "    tokens = [t for t in text.split() if t not in stop_words]\n",
    "    tokens = [stemmer.stem(t) for t in tokens]\n",
    "    \n",
    "    # remove short tokens consisting of less than 3 characters\n",
    "    tokens = [t for t in tokens if len(t) >= 3]\n",
    "\n",
    "    # generate 2-grams and 3-grams\n",
    "    bigrams = [tokens[i] + \" \" + tokens[i+1] for i in range(len(tokens)-1)]\n",
    "    trigrams = [tokens[i] + \" \" + tokens[i+1] + \" \" + tokens[i+2] for i in range(len(tokens) - 2)]\n",
    "    \n",
    "    return tokens + bigrams + trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f1d9b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_stemmed = TfidfVectorizer(\n",
    "    max_features=10000,\n",
    "    min_df=10,\n",
    "    max_df=0.5,\n",
    "    analyzer=stem_analyzer2,\n",
    "    token_pattern=None  # must be None when using a custom analyzer\n",
    ")\n",
    "text_col = df_stemmed.columns[1]\n",
    "tfidf_matrix_stemmed = vectorizer_stemmed.fit_transform(df_stemmed[text_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5248218f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['aaco', 'aaron', 'abalon', 'abandon', 'abar', 'abattoir', 'abba',\n",
       "       'abbot', 'abbot point', 'abbott', 'abbott say', 'abc',\n",
       "       'abc journalist', 'abc learn', 'abc news', 'abc news breakfast',\n",
       "       'abc news quiz', 'abc radio', 'abc report', 'abduct', 'abe',\n",
       "       'abetz', 'abil', 'abl', 'ablett', 'aboard', 'abolish', 'aborigin',\n",
       "       'aborigin communiti', 'aborigin elder', 'aborigin land', 'abort',\n",
       "       'absenc', 'absolut', 'abu', 'abu ghraib', 'abus', 'abus alleg',\n",
       "       'abus case', 'abus charg', 'abus claim', 'abus inquiri',\n",
       "       'abus report', 'abus royal', 'abus royal commiss', 'abus survivor',\n",
       "       'abus victim', 'academ', 'academi', 'accc', 'acceler', 'accept',\n",
       "       'access', 'accid', 'accident', 'accommod', 'accord', 'account',\n",
       "       'accredit', 'accus', 'accus assault', 'accus child', 'accus court',\n",
       "       'accus deni', 'accus drug', 'accus face', 'accus face court',\n",
       "       'accus govt', 'accus kill', 'accus murder', 'accus rape',\n",
       "       'accus tri', 'ace', 'aceh', 'achiev', 'acid', 'acknowledg', 'acl',\n",
       "       'acquir', 'acquisit', 'acquit', 'acquitt', 'act', 'act budget',\n",
       "       'act elect', 'act govern', 'act govt', 'act labor', 'act polic',\n",
       "       'action', 'action group', 'activ', 'activist', 'actor', 'actress',\n",
       "       'actu', 'actual', 'adam', 'adam gile', 'adam good'], dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer_stemmed.get_feature_names_out()[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a472b3ac",
   "metadata": {},
   "source": [
    "Now I think it seems better"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed02f124",
   "metadata": {},
   "source": [
    "After consideration and some noises in clustering I was thinking, I don't have enought dimension:\n",
    "\n",
    "So I increased SVD 200 -> 300: result was worse\n",
    "\n",
    "Now I am thinking maybe trigrams are causing some noise, and preventing from getting other useful features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f295d778",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "205"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigrams = [t for t in vectorizer_stemmed.get_feature_names_out()\n",
    "            if len(t.split()) == 3]\n",
    "\n",
    "len(trigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40650a06",
   "metadata": {},
   "source": [
    "We have only 206 trigrams, meaning they are few and rare, which means they are less likely affect 10K features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62d2f86",
   "metadata": {},
   "source": [
    "### 3_Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c6fa87f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# with open(\"../outputs/tfidf_matrix.pkl\", \"rb\") as f:\n",
    "#     tfidf_matrix = pickle.load(f)\n",
    "\n",
    "# with open(\"../outputs/tfidf_vectorizer.pkl\", \"rb\") as f:\n",
    "#     vectorizer = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83aa163",
   "metadata": {},
   "source": [
    "I wanted to check how much variance is preserved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "885d4244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 0.06605752025680846\n",
      "100 0.11248435875116039\n",
      "200 0.18521223483233745\n",
      "300 0.24384735173402503\n"
     ]
    }
   ],
   "source": [
    "for k in [50, 100, 200, 300]:\n",
    "    svd_k = TruncatedSVD(n_components=k, random_state=42, algorithm='randomized')\n",
    "    svd_k.fit(tfidf_matrix_stemmed)\n",
    "    print(k, svd_k.explained_variance_ratio_.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bf66ab",
   "metadata": {},
   "source": [
    "But it works too long, so next I'll do larger k numbers but with sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1bf66212",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "n_samples = 250000\n",
    "rows = np.random.choice(tfidf_matrix_stemmed.shape[0], size=n_samples, replace=False)\n",
    "tfidf_sample = tfidf_matrix_stemmed[rows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e3e923dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=100: explained variance = 0.1131\n",
      "k=200: explained variance = 0.1861\n",
      "k=300: explained variance = 0.2452\n",
      "k=400: explained variance = 0.2953\n",
      "k=500: explained variance = 0.3383\n"
     ]
    }
   ],
   "source": [
    "ks = [100, 200, 300, 400, 500]\n",
    "results = {}\n",
    "\n",
    "for k in ks:\n",
    "    svd_k = TruncatedSVD(n_components=k, random_state=42, algorithm='randomized')\n",
    "    svd_k.fit(tfidf_sample)\n",
    "    explained = svd_k.explained_variance_ratio_.sum()\n",
    "    results[k] = explained\n",
    "    print(f\"k={k}: explained variance = {explained:.4f}\")\n",
    "\n",
    "# Storing in svd_results.txt\n",
    "with open(\"../outputs/svd_results.txt\", \"w\") as f:\n",
    "    for k, val in results.items():\n",
    "        f.write(f\"k = {k}: explained variance = {val:.6f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b7e7d4",
   "metadata": {},
   "source": [
    "This evaluation with SAMPLE gives similar results as without it, so it is accurate.\n",
    "\n",
    "However, picking up larger k number would make further computations costly: \n",
    "\n",
    "So we would stick to 200, because 300 gives extra 6% variance but with trade-off 50% more dimensions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b56c177",
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=200, random_state=42, algorithm='randomized')\n",
    "lsa = svd.fit_transform(tfidf_matrix_stemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16fd46b",
   "metadata": {},
   "source": [
    "### Trying Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe16ff52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.01826689, -0.01919928, -0.00252245, ...,  0.00578481,\n",
       "         0.02331572,  0.00167638],\n",
       "       [ 0.02275818, -0.01223092,  0.00530972, ...,  0.00580389,\n",
       "         0.00394646, -0.01437576],\n",
       "       [ 0.03100368, -0.03363738, -0.00031488, ...,  0.02387418,\n",
       "         0.02904686,  0.02196581],\n",
       "       ...,\n",
       "       [ 0.06605609, -0.10435397, -0.02448211, ...,  0.00936457,\n",
       "        -0.0023124 ,  0.00544669],\n",
       "       [ 0.00909803, -0.00780246, -0.00199412, ..., -0.00671774,\n",
       "        -0.00829798, -0.00113839],\n",
       "       [ 0.0813937 , -0.11897488, -0.02816409, ...,  0.00999959,\n",
       "        -0.01723483,  0.01807941]], shape=(1213004, 200))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2c1807",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
