{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16b9a063",
   "metadata": {},
   "source": [
    "### 1_data_cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a62345ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publish_date</th>\n",
       "      <th>headline_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>116298</th>\n",
       "      <td>2004-09-20</td>\n",
       "      <td>10 killed in pakistan bus crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57967</th>\n",
       "      <td>2003-11-29</td>\n",
       "      <td>10 killed in pakistan bus crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>911080</th>\n",
       "      <td>2014-10-23</td>\n",
       "      <td>110 with barry nicholls</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>672958</th>\n",
       "      <td>2012-02-17</td>\n",
       "      <td>110 with barry nicholls</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>748629</th>\n",
       "      <td>2012-12-14</td>\n",
       "      <td>110 with barry nicholls</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       publish_date                    headline_text\n",
       "116298   2004-09-20  10 killed in pakistan bus crash\n",
       "57967    2003-11-29  10 killed in pakistan bus crash\n",
       "911080   2014-10-23          110 with barry nicholls\n",
       "672958   2012-02-17          110 with barry nicholls\n",
       "748629   2012-12-14          110 with barry nicholls"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "\n",
    "df = pd.read_csv(\"../abcnews-date-text.csv\")\n",
    "#change date format\n",
    "df[df.columns[0]] = pd.to_datetime(df[df.columns[0]], format=\"%Y%m%d\", errors=\"coerce\")\n",
    "# checlk for duplicates\n",
    "df[df['headline_text'].duplicated(keep=False)].sort_values('headline_text').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4c6d19b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicates\n",
    "df = df.drop_duplicates('headline_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4719efac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# work on a copy of the original dataframe\n",
    "df_stemmed = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bf59b7",
   "metadata": {},
   "source": [
    "### 2_feature_engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37020c3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publish_date</th>\n",
       "      <th>headline_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2003-02-19</td>\n",
       "      <td>aba decides against community broadcasting lic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2003-02-19</td>\n",
       "      <td>act fire witnesses must be aware of defamation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2003-02-19</td>\n",
       "      <td>a g calls for infrastructure protection summit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2003-02-19</td>\n",
       "      <td>air nz staff in aust strike for pay rise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2003-02-19</td>\n",
       "      <td>air nz strike to affect australian travellers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1244179</th>\n",
       "      <td>2021-12-31</td>\n",
       "      <td>two aged care residents die as state records ;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1244180</th>\n",
       "      <td>2021-12-31</td>\n",
       "      <td>victoria records ; new cases and seven deaths</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1244181</th>\n",
       "      <td>2021-12-31</td>\n",
       "      <td>wa delays adopting new close contact definition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1244182</th>\n",
       "      <td>2021-12-31</td>\n",
       "      <td>western ringtail possums found badly dehydrate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1244183</th>\n",
       "      <td>2021-12-31</td>\n",
       "      <td>what makes you a close covid contact here are ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1244184 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        publish_date                                      headline_text\n",
       "0         2003-02-19  aba decides against community broadcasting lic...\n",
       "1         2003-02-19     act fire witnesses must be aware of defamation\n",
       "2         2003-02-19     a g calls for infrastructure protection summit\n",
       "3         2003-02-19           air nz staff in aust strike for pay rise\n",
       "4         2003-02-19      air nz strike to affect australian travellers\n",
       "...              ...                                                ...\n",
       "1244179   2021-12-31     two aged care residents die as state records ;\n",
       "1244180   2021-12-31      victoria records ; new cases and seven deaths\n",
       "1244181   2021-12-31    wa delays adopting new close contact definition\n",
       "1244182   2021-12-31  western ringtail possums found badly dehydrate...\n",
       "1244183   2021-12-31  what makes you a close covid contact here are ...\n",
       "\n",
       "[1244184 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "\n",
    "text_col = df.columns[1]\n",
    "df[text_col] = df[text_col].apply(\n",
    "    lambda x: re.sub(r\"\\s+\", \" \", re.sub(r\"\\d+\", \"\", x.lower())).strip()\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8900b60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the TF-IDF matrix\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=10000,\n",
    "    min_df=10,\n",
    "    max_df=0.5,\n",
    "    ngram_range=(1, 3),\n",
    "    stop_words=\"english\",\n",
    "    token_pattern=r\"(?u)\\b[a-zA-Z]{2,}\\b\",\n",
    ")\n",
    "tfidf_matrix = vectorizer.fit_transform(df[text_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e215e3fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aaco' 'aaron' 'abalone' 'abandon' 'abandoned' 'abandons' 'abares'\n",
      " 'abattoir' 'abbas' 'abbot' 'abbot point' 'abbott' 'abbott says' 'abbotts'\n",
      " 'abc' 'abc business' 'abc business news' 'abc entertainment'\n",
      " 'abc learning' 'abc news' 'abc news breakfast' 'abc news quiz'\n",
      " 'abc reporter' 'abc sport' 'abc weather' 'abcs' 'abducted' 'abduction'\n",
      " 'abe' 'abetz' 'able' 'ablett' 'aboard' 'aboriginal'\n",
      " 'aboriginal community' 'aborigines' 'abortion' 'abs' 'absence' 'abu'\n",
      " 'abuse' 'abuse claims' 'abuse inquiry' 'abuse victims' 'abused' 'abuses'\n",
      " 'abusing' 'academic' 'academy' 'accc' 'accept' 'accepts' 'access'\n",
      " 'accident' 'accidental' 'accidentally' 'accidents' 'accommodation'\n",
      " 'accord' 'account' 'accounts' 'accreditation' 'accusations' 'accuse'\n",
      " 'accused' 'accused child' 'accused court' 'accused face'\n",
      " 'accused killing' 'accuses' 'aceh' 'acid' 'acknowledges' 'acl' 'acquired'\n",
      " 'acquisition' 'acquitted' 'act' 'act budget' 'act election'\n",
      " 'act government' 'act govt' 'act police' 'acted' 'acting' 'action'\n",
      " 'action group' 'actions' 'active' 'activist' 'activists' 'activity'\n",
      " 'activity analysis' 'actor' 'actors' 'actress' 'acts' 'actu' 'ad'\n",
      " 'ad campaign']\n"
     ]
    }
   ],
   "source": [
    "terms = vectorizer.get_feature_names_out()\n",
    "print(terms[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defc9822",
   "metadata": {},
   "source": [
    "### Improving\n",
    "\n",
    "As you can see, here we have words with same stem: abandon, abandoned, abandons.\n",
    "So next I will use stemming with nltk (Natural Language Toolkit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bc3292",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "import re\n",
    "\n",
    "stop_words = ENGLISH_STOP_WORDS\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def stem_analyzer(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"\\d+\", \" \", text)            # remove digits\n",
    "    text = re.sub(r\"[^a-z\\s]\", \" \", text)       # remove punctuation and symbols\n",
    "    text = re.sub(r\"\\s+\", \" \", text)          # normalize whitespace\n",
    "    tokens = text.split()\n",
    "    tokens = [t for t in text.split() if t not in stop_words]\n",
    "    tokens = [stemmer.stem(t) for t in tokens]\n",
    "\n",
    "    # generate 2-grams and 3-grams\n",
    "    bigrams = [tokens[i] + \" \" + tokens[i+1] for i in range(len(tokens)-1)]\n",
    "    trigrams = [tokens[i] + \" \" + tokens[i+1] + \" \" + tokens[i+2] for i in range(len(tokens) - 2)]\n",
    "    \n",
    "    return tokens + bigrams + trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcdd663",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_stemmed = TfidfVectorizer(\n",
    "    max_features=10000,\n",
    "    min_df=10,\n",
    "    max_df=0.5,\n",
    "    analyzer=stem_analyzer,\n",
    "    token_pattern=None  # must be None when using a custom analyzer\n",
    ")\n",
    "text_col = df_stemmed.columns[1]\n",
    "tfidf_matrix_stemmed = vectorizer_stemmed.fit_transform(df_stemmed[text_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd5bc77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aa' 'aaa' 'aaa credit' 'aaa credit rate' 'aaa rate' 'aaco'\n",
      " 'aaco abattoir' 'aacta' 'aacta award' 'aamer' 'aami' 'aapt' 'aaron'\n",
      " 'aaron finch' 'aaron mooy' 'aaron pajich' 'aaron pajich murder'\n",
      " 'aaron payn' 'aaron sandiland' 'aaron wood' 'ab' 'ab data' 'ab figur'\n",
      " 'ab job' 'ab june' 'ab villier' 'aba' 'abalon' 'abalon diver'\n",
      " 'abalon farm' 'abalon fish' 'abalon fisher' 'abalon haul'\n",
      " 'abalon industri' 'abalon poach' 'abalon poacher' 'abalon season'\n",
      " 'abalon virus' 'abandon' 'abandon babi' 'abandon car' 'abandon hous'\n",
      " 'abandon plan' 'abandon toddler' 'abar' 'abar crop' 'abar forecast'\n",
      " 'abar outlook' 'abar predict' 'abat' 'abattoir' 'abattoir close'\n",
      " 'abattoir closur' 'abattoir get' 'abattoir open' 'abattoir owner'\n",
      " 'abattoir plan' 'abattoir reopen' 'abattoir worker' 'abb' 'abb grain'\n",
      " 'abba' 'abbatoir' 'abbey' 'abbey road' 'abbi' 'abbot' 'abbot point'\n",
      " 'abbot point coal' 'abbot point dredg' 'abbot point expans' 'abbott'\n",
      " 'abbott accus' 'abbott address' 'abbott announc' 'abbott arriv'\n",
      " 'abbott attack' 'abbott back' 'abbott blame' 'abbott budget'\n",
      " 'abbott call' 'abbott campaign' 'abbott carbon' 'abbott carbon tax'\n",
      " 'abbott claim' 'abbott climat' 'abbott commit' 'abbott confid'\n",
      " 'abbott consid' 'abbott criticis' 'abbott defend' 'abbott deliv'\n",
      " 'abbott deni' 'abbott dismiss' 'abbott face' 'abbott flag'\n",
      " 'abbott govern' 'abbott launch' 'abbott make' 'abbott meet']\n",
      "(91607,)\n"
     ]
    }
   ],
   "source": [
    "terms_stemmed = vectorizer_stemmed.get_feature_names_out()\n",
    "print(terms_stemmed[:100])\n",
    "print(terms_stemmed.shape)\n",
    "\n",
    "# checked shape wihtout max_features parameter: 91607 features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df1220e",
   "metadata": {},
   "source": [
    "'aa' 'ab' 'ab data' 'ab figur' 'ab job' 'ab june' 'ab villier' 'aba'\n",
    "\n",
    "I think we should remove terms consisting less than 2 letters even after stemming it seems like random noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "824e2eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_analyzer2(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"\\d+\", \" \", text)            # remove digits\n",
    "    text = re.sub(r\"[^a-z\\s]\", \" \", text)       # remove punctuation and symbols\n",
    "    text = re.sub(r\"\\s+\", \" \", text)          # normalize whitespace\n",
    "    tokens = text.split()\n",
    "    tokens = [t for t in text.split() if t not in stop_words]\n",
    "    tokens = [stemmer.stem(t) for t in tokens]\n",
    "    \n",
    "    # remove short tokens consisting of less than 3 characters\n",
    "    tokens = [t for t in tokens if len(t) >= 3]\n",
    "\n",
    "    # generate 2-grams and 3-grams\n",
    "    bigrams = [tokens[i] + \" \" + tokens[i+1] for i in range(len(tokens)-1)]\n",
    "    trigrams = [tokens[i] + \" \" + tokens[i+1] + \" \" + tokens[i+2] for i in range(len(tokens) - 2)]\n",
    "    \n",
    "    return tokens + bigrams + trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0f1d9b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_stemmed = TfidfVectorizer(\n",
    "    max_features=10000,\n",
    "    min_df=10,\n",
    "    max_df=0.5,\n",
    "    analyzer=stem_analyzer2,\n",
    "    token_pattern=None  # must be None when using a custom analyzer\n",
    ")\n",
    "text_col = df_stemmed.columns[1]\n",
    "tfidf_matrix_stemmed = vectorizer_stemmed.fit_transform(df_stemmed[text_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5248218f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['aaco', 'aaron', 'abalon', 'abandon', 'abar', 'abattoir', 'abba',\n",
       "       'abbot', 'abbot point', 'abbott', 'abbott say', 'abc',\n",
       "       'abc journalist', 'abc learn', 'abc news', 'abc news breakfast',\n",
       "       'abc news quiz', 'abc radio', 'abc report', 'abduct', 'abe',\n",
       "       'abetz', 'abil', 'abl', 'ablett', 'aboard', 'abolish', 'aborigin',\n",
       "       'aborigin communiti', 'aborigin elder', 'aborigin land', 'abort',\n",
       "       'absenc', 'absolut', 'abu', 'abu ghraib', 'abus', 'abus alleg',\n",
       "       'abus case', 'abus charg', 'abus claim', 'abus inquiri',\n",
       "       'abus report', 'abus royal', 'abus royal commiss', 'abus survivor',\n",
       "       'abus victim', 'academ', 'academi', 'accc', 'acceler', 'accept',\n",
       "       'access', 'accid', 'accident', 'accommod', 'accord', 'account',\n",
       "       'accredit', 'accus', 'accus assault', 'accus child', 'accus court',\n",
       "       'accus deni', 'accus drug', 'accus face', 'accus face court',\n",
       "       'accus govt', 'accus kill', 'accus murder', 'accus rape',\n",
       "       'accus tri', 'ace', 'aceh', 'achiev', 'acid', 'acknowledg', 'acl',\n",
       "       'acquir', 'acquisit', 'acquit', 'acquitt', 'act', 'act budget',\n",
       "       'act elect', 'act govern', 'act govt', 'act labor', 'act polic',\n",
       "       'action', 'action group', 'activ', 'activist', 'actor', 'actress',\n",
       "       'actu', 'actual', 'adam', 'adam gile', 'adam good'], dtype=object)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer_stemmed.get_feature_names_out()[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a472b3ac",
   "metadata": {},
   "source": [
    "Now I think it seems better"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed02f124",
   "metadata": {},
   "source": [
    "After consideration and some noises in clustering I was thinking, I don't have enought dimension:\n",
    "\n",
    "So I increased SVD 200 -> 300: result was worse\n",
    "\n",
    "Now I am thinking maybe trigrams are causing some noise, and preventing from getting other useful features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f295d778",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "206"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigrams = [t for t in vectorizer_stemmed.get_feature_names_out()\n",
    "            if len(t.split()) == 3]\n",
    "\n",
    "len(trigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40650a06",
   "metadata": {},
   "source": [
    "We have only 206 trigrams, meaning they are few and rare, which means they are less likely affect 10K features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62d2f86",
   "metadata": {},
   "source": [
    "### 3_Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fa87f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# with open(\"../outputs/tfidf_matrix.pkl\", \"rb\") as f:\n",
    "#     tfidf_matrix = pickle.load(f)\n",
    "\n",
    "# with open(\"../outputs/tfidf_vectorizer.pkl\", \"rb\") as f:\n",
    "#     vectorizer = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83aa163",
   "metadata": {},
   "source": [
    "I wanted to check how much variance is preserved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "885d4244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 0.06598297508201635\n",
      "100 0.11256041703815221\n",
      "200 0.18517767080470943\n",
      "300 0.2439157964019349\n"
     ]
    }
   ],
   "source": [
    "for k in [50, 100, 200, 300]:\n",
    "    svd_k = TruncatedSVD(n_components=k, random_state=42)\n",
    "    svd_k.fit(tfidf_matrix_stemmed)\n",
    "    print(k, svd_k.explained_variance_ratio_.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bf66ab",
   "metadata": {},
   "source": [
    "But it works too long, so next I'll do larger k numbers but with sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1bf66212",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "n_samples = 250000\n",
    "rows = np.random.choice(tfidf_matrix_stemmed.shape[0], size=n_samples, replace=False)\n",
    "tfidf_sample = tfidf_matrix_stemmed[rows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e923dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=100: explained variance = 0.1131\n",
      "k=200: explained variance = 0.1857\n",
      "k=300: explained variance = 0.2447\n",
      "k=400: explained variance = 0.2944\n",
      "k=500: explained variance = 0.3373\n"
     ]
    }
   ],
   "source": [
    "ks = [100, 200, 300, 400, 500]\n",
    "results = {}\n",
    "\n",
    "for k in ks:\n",
    "    svd_k = TruncatedSVD(n_components=k, random_state=42)\n",
    "    svd_k.fit(tfidf_sample)\n",
    "    explained = svd_k.explained_variance_ratio_.sum()\n",
    "    results[k] = explained\n",
    "    print(f\"k={k}: explained variance = {explained:.4f}\")\n",
    "\n",
    "# Storing in svd_results.txt\n",
    "with open(\"../outputs/svd_results.txt\", \"w\") as f:\n",
    "    for k, val in results.items():\n",
    "        f.write(f\"k = {k}: explained variance = {val:.6f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b7e7d4",
   "metadata": {},
   "source": [
    "This evaluation with SAMPLE gives similar results as without it, so it is accurate.\n",
    "\n",
    "However, picking up larger k number would make further computations costly: \n",
    "\n",
    "So we would stick to 200, because 300 gives extra 6% variance but with trade-off 50% more dimensions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3b56c177",
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=200, random_state=42)\n",
    "lsa = svd.fit_transform(tfidf_matrix_stemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf16fe67",
   "metadata": {},
   "source": [
    "### 4 Clustering_and_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe16ff52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1213004, 200)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2c1807",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
